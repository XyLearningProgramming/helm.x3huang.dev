grafana:
  enabled: false

alertmanager:
  enabled: false

defaultRules:
  create: false

kubeEtcd:
  enabled: false

kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

kubeApiServer:
  enabled: false

kubelet:
  enabled: true
  serviceMonitor:
    enabled: true
    kubelet: true
    interval: "59s"

kubeProxy:
  enabled: false

# CoreDNS relies on default config, good enough.
#
coreDns:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: "67s"
# additionalServiceMonitors:
#   - name: coredns
#     namespaceSelector:
#       matchNames: ["kube-system"]
#     selector:
#       matchLabels:
#         k8s-app: kube-dns
#     endpoints:
#       - port: metrics
#         interval: 1m

# For node-exporter, default is and I'd like to add host machine metrics in addition:
# Ref: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
# 
# Enable node exporter to collect host metrics
prometheus-node-exporter:
  prometheus:
    monitor:
      enabled: true
      interval: "61s"
  extraArgs:
    - --collector.processes
    - --collector.cpu
    - --collector.meminfo
    - --collector.netdev
    - --collector.loadavg
    - --collector.diskstats
    # - --collector.systemd # commented out, not mounting /var/run/dbus for now
    # - --collector.filesystem

prometheus:
  enabled: true
  prometheusSpec:
    logLevel: {{ if eq .Environment.Name "prod" }}info{{ else }}debug{{ end }}
    scrapeInterval: 1m    # Global scrape interval
    maximumStartupDurationSeconds: 600  # Add this to fix the startup duration error
    retention: 3d         # Keep data for 5 days
    retentionSize: 8GB    # Maximum disk space to use
    # Prevent default limits from being added
    sampleLimit: 50000           # Maximum number of samples per scrape
    targetLimit: 1000           # Maximum number of targets to scrape
    labelLimit: 1000           # Maximum number of labels per series
    labelNameLengthLimit: 1024  # Maximum length of label names
    labelValueLengthLimit: 2048 # Maximum length of label values
    resources:
      requests: # Good practice
        memory: "256Mi"
        cpu: "200m" # 0.2 CPU
      limits:
        memory: "512Mi"  # Start with 0.5Gi, monitor and adjust
        cpu: "500m"       # Start with 0.5 CPU, monitor and adjust
    remoteWrite:
      - url: https://prometheus-prod-37-prod-ap-southeast-1.grafana.net/api/prom/push
        remoteTimeout: 3m
        basicAuth:
          username:
            name: helmfile-secret-monitoring
            key: GRAFANA_CLOUD_USERNAME
          password:
            name: helmfile-secret-monitoring
            key: GRAFANA_CLOUD_API_KEY
        queueConfig:
          batchSendDeadline: 1m          # Same as scrapeInterval
          maxSamplesPerSend: 200         # Increased from 100 to reduce number of API calls
          capacity: 5000                 # Increased from 5000 to handle more samples
          maxShards: 7                   # Increased from 3 to handle higher throughput, 200*7 < 1500/s as limit
          minShards: 1                    # Increased from 1 to ensure minimum throughput
          retryOnRateLimit: true
        writeRelabelConfigs:
            # Reduce cardinality by this usage report
            # 
            # etcd_request_duration_seconds_bucket
            # 191912.8%unused
            # apiserver_request_body_size_bytes_bucket
            # 10406.93%unused
            # workqueue_queue_duration_seconds_bucket
            # 10016.67%unused
            # workqueue_work_duration_seconds_bucket
            # 10016.67%unused
            # scheduler_plugin_execution_duration_seconds_bucket
            # 6934.62%unused
            # apiserver_request_sli_duration_seconds_bucket
            # 5503.67%unused
            # apiserver_admission_controller_admission_duration_seconds_bucket
            # 3432.29%unused
            # etcd_requests_total
            # 2851.90%unused
            # apiserver_flowcontrol_request_execution_seconds_bucket
            # 2801.87%unused
          {{- $dropPrefixes := list 
              "etcd"
              "apiserver"
              "workqueue"
              "scheduler"
          }}
          - sourceLabels: ["__name__"]
            regex: {{ printf "^(?:%s)_.*" (join "|" $dropPrefixes) }}
            action: drop
          - targetLabel: domain
            replacement: {{ .Values | get "DOMAIN" }}
            action: replace
    # Allow Prometheus to discover all PodMonitors/ServiceMonitors within its namespace, 
    # without applying label filtering
    # Ref: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack#prometheus-io-scrape
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}
